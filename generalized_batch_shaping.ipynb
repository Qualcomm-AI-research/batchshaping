{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "345b3737-3f77-4a04-8f9d-8eb9e9fe3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2023 Qualcomm Technologies, Inc.\n",
    "# All Rights Reserved.\n",
    "\n",
    "\"\"\"Generalized Batch-Shaping example notebook\"\"\"\n",
    "\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "from batchshaping.batch_shaping_loss import Prior, gbas_loss\n",
    "from batchshaping.utils import set_seed, to_numpy, warmup_factory\n",
    "from batchshaping.viz_utils import init_gbas_anim_plot, plot_gt_cdf, plot_gt_pdf\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb16f7d-e004-4dc3-af92-b81b223c7cb4",
   "metadata": {},
   "source": [
    "# Generalized Batch-Shaping\n",
    "\n",
    "\n",
    "### Introduction\n",
    "The original Batch-Shaping loss only acts on 1-dimensional inputs $x$ which can be a limiting factor: For instance in the case of gated networks, we may want to impose a different sparsity prior (hence a different CDF in the Batch-Shaping loss) for each layer. In such cases, we can instead minimize a sum of CDF losses:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{BaS,multi}(x^1, \\dots x^T) = \\sum_{k=1}^{T} \\frac{1}{N} \\sum_{i=1}^N \\left( \\hat{F}(x^k_i) - F^{\\ast, k}(x^k_{i}; \\phi^k) \\right)^2\n",
    "$$\n",
    "\n",
    "However, defining the target prior parameters for each dimension, ($\\phi^k$) is cumbersome and assumes a lot of prior knowledge. Instead, we introduce the **generalized Batch-Shaping loss** (gBaS) in which the priors' parameters are learned, and controlled by an additional hyperprior: \n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{hyperprior}(\\phi) = \\frac{1}{T} \\sum_{k=1}^T \\left( \\hat{F}(\\phi^k) - \\mathcal{F}^{\\ast}(\\phi^k; \\psi) \\right)^2\n",
    "$$\n",
    "\n",
    "where $\\hat{F}$ still indicates the empirical CDF, and $\\mathcal{F}$ is a prior over the parameters $\\phi$, with its own parameters $\\psi$.\n",
    "\n",
    "Intuitively, each prior $F^{\\ast, k}$ controls the token sparsity for a given local position, where the mean and temperature of the prior are learned latent parameters. Then, the hyperprior $\\mathcal{F}$ is a Gaussian whose mean controls  the overall sparsity across all positions, and variance controls how far the prior for each position can be from one another. We introduce gBaS in more details in [MSViT: Dynamic Mixed-Scale Tokenization for Vision Transformers, Havtorn et al, arXiv 2023](http://arxiv.org/abs/2307.02321).\n",
    "\n",
    "### A practical example\n",
    "Next, we illustrate the generalized Batch-Shaping loss on an example. Say we have $T$ layers, each with their own gate, outputting a $[0, 1]$ value for each of the $N$ samples in the batch. \n",
    "For each layer, we want the distribution of the gated outputs, $x^k$ to follow a certain Relaxed Bernoulli (RB) distribution (i.e. the mass of the distribution should be located around the discrete peaks $\\{0, 1\\}$). \n",
    "We do not have a prior estimate for each layer individually, thus we let the model learn the mean of each RB prior. However, we do have a target gate sparsity (in this example, 30%), which we use to define the hyperprior controlling the learned means. \n",
    "\n",
    "In summary:\n",
    "  * The learned data $x^k$'s distribution is controlled by a Relaxed Bernoulli $RB(\\mu^k, \\tau^k)$ for each layer\n",
    "  * The distribution of the means $\\mu$ is controlled by a hyperprior with a mean equal to the sparsity target and a given variance hyperparameter controlling the spread across layers (in the example, we use a Beta hyperprior $B(3, 7)$).\n",
    "  * The temperatures $\\tau^k$ are also learnable but do not have a hyperprior term: We only add a $L_1$ loss term on $\\tau$ to favor more peaky distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d8fe62f-15fe-4470-adcc-ee53a690c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "NUM_DIMENSIONS = 30  # T\n",
    "NUM_POINTS_PER_DIM = 500  # N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f46c0b5c-1761-4644-ae5e-0995c9ebdc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gbas(\n",
    "    num_dimensions: int,\n",
    "    num_points_per_dim: int,\n",
    "    num_epochs: int,\n",
    "    hyperprior: Prior,\n",
    "    hyperprior_param1: float,\n",
    "    hyperprior_param2: Optional[float] = None,\n",
    "    temperature_l0_lw: float = 0.0,\n",
    "    num_warmup_epochs: int = 0,\n",
    "    lr: float = 0.1,\n",
    "    init_range: float = 4,\n",
    ") -> FuncAnimation:\n",
    "    \"\"\"Train generalized Batch-Shaping loss example\"\"\"\n",
    "    # Init plot\n",
    "    (\n",
    "        fig,\n",
    "        data_axes,\n",
    "        priors_loss_ax,\n",
    "        hyperprior_loss_ax,\n",
    "        hyperprior_pdf_ax,\n",
    "        priors_loss_plot,\n",
    "        hyperprior_loss_plot,\n",
    "    ) = init_gbas_anim_plot(num_epochs)\n",
    "    priors_losses = []\n",
    "    hyperprior_losses = []\n",
    "\n",
    "    # Init data points randomly for each dimension\n",
    "    data_list = []\n",
    "    for _ in range(num_dimensions):\n",
    "        mean = np.random.rand()\n",
    "        data_list.append(\n",
    "            (np.clip(np.random.normal(mean, 0.05, size=num_points_per_dim), 0.0, 1.0) * 2 - 1)\n",
    "            * init_range\n",
    "        )\n",
    "    data = np.stack(data_list, axis=1)\n",
    "    data_ys = np.linspace(0.0, 1.0, data.shape[0])\n",
    "    data = torch.nn.Parameter(data=torch.Tensor(data), requires_grad=True)\n",
    "\n",
    "    # Learnable parameters\n",
    "    # initialize priors mean around 0.5\n",
    "    # and with a large enough temperature (also 0.5)\n",
    "    prior = Prior.RELAXED_BERNOULLI\n",
    "    priors_means = torch.nn.Parameter(\n",
    "        data=torch.normal(torch.zeros(num_dimensions), 0.01), requires_grad=True\n",
    "    )\n",
    "    priors_temperatures = torch.nn.Parameter(data=torch.zeros(num_dimensions), requires_grad=True)\n",
    "\n",
    "    # Init optimizer\n",
    "    optimizer = torch.optim.Adam([data, priors_means, priors_temperatures], lr=lr)\n",
    "    lr_warmup = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer, lr_lambda=warmup_factory(num_warmup_epochs, lr)\n",
    "    )\n",
    "\n",
    "    # control the data points across each distribution with learned means and temperatures\n",
    "    prior_loss_fn = partial(gbas_loss, prior=prior, dim=1)\n",
    "\n",
    "    # control the learned means of the priors\n",
    "    hyperprior_loss_fn = partial(\n",
    "        gbas_loss,\n",
    "        prior=hyperprior,\n",
    "        prior_param1=hyperprior_param1,\n",
    "        prior_param2=hyperprior_param2,\n",
    "        dim=None,\n",
    "    )\n",
    "    # Plot init\n",
    "\n",
    "    def init_func() -> None:\n",
    "        loss = prior_loss_fn(\n",
    "            torch.sigmoid(data),\n",
    "            prior_param1=torch.sigmoid(priors_means),\n",
    "            prior_param2=torch.sigmoid(priors_temperatures),\n",
    "        )\n",
    "        priors_loss_ax.set_ylim([-0.005, loss.item() + 0.05])\n",
    "        loss = hyperprior_loss_fn(torch.sigmoid(priors_means))\n",
    "        hyperprior_loss_ax.set_ylim([-0.005, loss.item() + 0.05])\n",
    "\n",
    "    # Train + Plot update\n",
    "    def one_step(i: int) -> None:\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        out = torch.sigmoid(data)\n",
    "        means = torch.sigmoid(priors_means)\n",
    "        temps = torch.sigmoid(priors_temperatures)\n",
    "        ploss = prior_loss_fn(out, prior_param1=means, prior_param2=temps)\n",
    "        hloss = hyperprior_loss_fn(means)\n",
    "        loss = ploss + hloss + temperature_l0_lw * torch.mean(torch.abs(temps))\n",
    "        print(\n",
    "            f\"\\r[step {i + 1:04d} / {num_epochs:04d}] loss = {loss.item():.2e}\",\n",
    "            end=\"\" if i < num_epochs - 1 else \"\\n\",\n",
    "        )\n",
    "        priors_losses.append(ploss.item())\n",
    "        hyperprior_losses.append(hloss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_warmup.step()\n",
    "\n",
    "        # update plots\n",
    "        priors_loss_plot.set_data(np.arange(i + 1), priors_losses[: i + 1])\n",
    "        hyperprior_loss_plot.set_data(np.arange(i + 1), hyperprior_losses[: i + 1])\n",
    "        out_arr = to_numpy(out)\n",
    "        cdf_x = np.sort(out_arr, axis=0)\n",
    "        cdf_y = np.arange(cdf_x.shape[0]) / cdf_x.shape[0]\n",
    "        for idx, data_ax in enumerate(data_axes):\n",
    "            data_ax.cla()\n",
    "            m, tau = means[idx].item(), temps[idx].item()\n",
    "            data_ax.scatter(\n",
    "                out_arr[:, idx], data_ys, marker=\"o\", alpha=0.15, label=\"data\", color=\"orchid\"\n",
    "            )\n",
    "            data_ax.plot(\n",
    "                cdf_x[:, idx],\n",
    "                cdf_y,\n",
    "                linewidth=2.5,\n",
    "                linestyle=\"dashed\",\n",
    "                color=\"xkcd:turquoise\",\n",
    "                label=\"CDF (data)\",\n",
    "            )\n",
    "            plot_gt_cdf(data_ax, prior, m, tau)\n",
    "            data_ax.set_xlabel(\n",
    "                f\"Dim {idx + 1} \" + r\"($\\mu$=\" + f\"{m:.2f},\" + r\" $\\tau$=\" + f\"{tau:.2f})\",\n",
    "                fontsize=14,\n",
    "            )\n",
    "            if idx == 0:\n",
    "                data_ax.legend(loc=\"upper center\", ncol=3, bbox_to_anchor=(1.2, 1.28))\n",
    "        hyperprior_pdf_ax.cla()\n",
    "        plot_gt_pdf(hyperprior_pdf_ax, hyperprior, hyperprior_param1, hyperprior_param2)\n",
    "        hyperprior_pdf_ax.hist(\n",
    "            to_numpy(means),\n",
    "            label=\"PDF (data)\",\n",
    "            color=\"xkcd:turquoise\",\n",
    "            rwidth=0.9,\n",
    "            bins=20,\n",
    "            density=True,\n",
    "            stacked=True,\n",
    "        )\n",
    "        hyperprior_pdf_ax.legend(loc=\"upper right\")\n",
    "        hyperprior_pdf_ax.set_ylabel(\"(Hyperprior)\\n\" + r\"PDF of learned $\\mu$s\", fontsize=16)\n",
    "\n",
    "    ani = FuncAnimation(fig, one_step, frames=num_epochs, init_func=init_func)\n",
    "    plt.close()\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ca1ad47-7edb-4245-9858-ff8e5e018de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter ffmpeg unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 0350 / 0350] loss = 3.52e-02\n",
      "CPU times: user 6min 44s, sys: 1min 2s, total: 7min 46s\n",
      "Wall time: 5min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "HYPERPRIOR = Prior.BETA\n",
    "BETA_A = 3\n",
    "BETA_B = 7\n",
    "\n",
    "NUM_EPOCHS = 350\n",
    "NUM_WARMUP_EPOCHS = 20\n",
    "LR = 0.3\n",
    "TEMPERATURE_L0_LW = 0.1\n",
    "\n",
    "ANI = train_gbas(\n",
    "    NUM_DIMENSIONS,\n",
    "    NUM_POINTS_PER_DIM,\n",
    "    NUM_EPOCHS,\n",
    "    HYPERPRIOR,\n",
    "    BETA_A,\n",
    "    BETA_B,\n",
    "    temperature_l0_lw=TEMPERATURE_L0_LW,\n",
    "    num_warmup_epochs=NUM_WARMUP_EPOCHS,\n",
    "    lr=LR,\n",
    ")\n",
    "ANI.save(\n",
    "    f\"outputs/generalized_batch_shaping_hyperprior={HYPERPRIOR.name.lower()}\"\n",
    "    f\"_p1={BETA_A}_p2={BETA_B}.gif\",\n",
    "    fps=25,\n",
    "    dpi=60,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f074bd01-abec-4d91-8014-78c384ff1a29",
   "metadata": {},
   "source": [
    "![generalized_batch_shaping_hyperprior=beta_p1=3_p2=7.gif](outputs/generalized_batch_shaping_hyperprior=beta_p1=3_p2=7.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
