{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b4b6881-a0a8-4c4b-9b18-6739437e7ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2023 Qualcomm Technologies, Inc.\n",
    "# All Rights Reserved.\n",
    "\n",
    "\"\"\"Batch-Shaping example notebook\"\"\"\n",
    "\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "from batchshaping.batch_shaping_loss import Prior, gbas_loss\n",
    "from batchshaping.utils import set_seed, to_numpy\n",
    "from batchshaping.viz_utils import init_anim_plot, plot_gt_cdf, plot_gt_pdf\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602c2bc-b8ba-40fa-89ae-2774e2b26b6f",
   "metadata": {},
   "source": [
    "# Batch-Shaping\n",
    "\n",
    "The **Batch-Shaping loss** (BaS) is a probability distribution matching tool derived from the Cramér–von Mises goodness of fit criterion. It minimizes the difference between the cumulative distribution function (CDF) of the target distribution, and the empirical CDF of the current data points: \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x) = \\frac{1}{N} \\sum_{i=1}^N \\left( \\hat{F}(x_i) - F^\\ast(x_{i}; \\phi) \\right)^2\n",
    "$$\n",
    "\n",
    "where $i$ is the sample index, $N$ is the number of samples, $F^\\ast$ is the CDF of the target prior distribution with parameters $\\phi$, and $\\hat{F}$ is the empirical CDF of the data, which can be estimated as $\\hat{F}(x_i) = \\frac{i}{N}$, assuming that  $x$ is sorted.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "See [Batch-Shaping Loss for Learning Conditional-Channel Gated Networks, Bejnordi et al, ICLR 2020](https://arxiv.org/abs/1907.06627) and [MSViT: Dynamic Mixed-Scale Tokenization for Vision Transformers, Havtorn et al, arXiv 2023](http://arxiv.org/abs/2307.02321) for more details.\n",
    "\n",
    "\n",
    "## A. The Batch-Shaping loss in practice\n",
    "\n",
    "In the following examples, we illustrate how the Batch-Shaping loss can be used to match set of data points $x \\in [0, 1]$ to a Relaxed Bernoulli distribution (aka. Binary Concrete) or the Beta distribution. Each plot depicts the evolution of the data points (on the $x$ axis with a random ordinate value for readability) and the empirical CDF. The two bottom plots represent the evolution of the training loss and empirical probability density function (PDF) at the same time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ebca80a-9fc6-4114-83e1-4901b5b6d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "NUM_POINTS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ed59420-680f-40cf-95b3-74efc1811391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bas(\n",
    "    num_points: int,\n",
    "    num_epochs: int,\n",
    "    prior: Prior,\n",
    "    prior_param1: float,\n",
    "    prior_param2: Optional[float] = None,\n",
    "    lr: float = 0.1,\n",
    "    init_range: float = 4,\n",
    ") -> FuncAnimation:\n",
    "    \"\"\"Train Batch-Shaping loss example\"\"\"\n",
    "    # Init plot\n",
    "    fig, data_ax, loss_ax, pdf_ax, data_plot, cdf_pred_plot, loss_plot = init_anim_plot(num_epochs)\n",
    "    losses = []\n",
    "\n",
    "    def init_func() -> None:\n",
    "        plot_gt_cdf(data_ax, prior, prior_param1, prior_param2)\n",
    "        data_ax.legend(loc=\"upper center\", ncol=3, bbox_to_anchor=(0.5, 1.09))\n",
    "        out = torch.sigmoid(data)\n",
    "        loss = loss_fn(out[None, :])\n",
    "        loss_ax.set_ylim([-0.005, loss.item() + 0.05])\n",
    "\n",
    "    # Init data points (will be fed as input to sigmoid for support of [0, 1])\n",
    "    data = (\n",
    "        torch.rand(num_points, device=torch.device(\"cpu\"), requires_grad=True) * 2 - 1\n",
    "    ) * init_range\n",
    "    data_ys = np.linspace(0.0, 1.0, data.shape[0])\n",
    "    data = torch.nn.Parameter(data, requires_grad=True)\n",
    "\n",
    "    # Init optimizer\n",
    "    optimizer = torch.optim.Adam([data], lr=lr)\n",
    "    loss_fn = partial(\n",
    "        gbas_loss, prior=prior, prior_param1=prior_param1, prior_param2=prior_param2, dim=None\n",
    "    )\n",
    "\n",
    "    # Train update\n",
    "    def one_step(i: int) -> None:\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        out = torch.sigmoid(data)\n",
    "        loss = loss_fn(out)\n",
    "        print(\n",
    "            f\"\\r[step {i + 1:04d} / {num_epochs:04d}] loss = {loss.item():.2e}\",\n",
    "            end=\"\" if i < num_epochs - 1 else \"\\n\",\n",
    "        )\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update plots\n",
    "        loss_plot.set_data(np.arange(i + 1), losses[: i + 1])\n",
    "        data_plot.set_offsets(np.vstack((to_numpy(out), data_ys)).T)\n",
    "        cdf_x = np.sort(to_numpy(out))\n",
    "        cdf_y = np.arange(cdf_x.shape[0]) / cdf_x.shape[0]\n",
    "        cdf_pred_plot.set_data(cdf_x, cdf_y)\n",
    "        pdf_ax.cla()\n",
    "        plot_gt_pdf(pdf_ax, prior, prior_param1, prior_param2)\n",
    "        pdf_ax.hist(\n",
    "            cdf_x,\n",
    "            label=\"PDF (data)\",\n",
    "            color=\"xkcd:turquoise\",\n",
    "            rwidth=0.9,\n",
    "            bins=20,\n",
    "            density=True,\n",
    "            stacked=True,\n",
    "        )\n",
    "        pdf_ax.legend(loc=\"upper center\")\n",
    "\n",
    "    ani = FuncAnimation(fig, one_step, frames=num_epochs, init_func=init_func)\n",
    "    plt.close()\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd0e92f-32e0-4679-ab79-a17f911d3461",
   "metadata": {},
   "source": [
    "#### Example 1: Relaxed Bernoulli with mean 0.2 and temperature 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81fbb01-6891-4113-9aea-0105c172f403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter ffmpeg unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 0100 / 0100] loss = 2.38e-02\n",
      "CPU times: user 28.5 s, sys: 721 ms, total: 29.2 s\n",
      "Wall time: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NUM_EPOCHS = 100\n",
    "LR = 0.1\n",
    "PRIOR = Prior.RELAXED_BERNOULLI\n",
    "MEAN = 0.2\n",
    "TEMPERATURE = 0.25\n",
    "\n",
    "ANI = train_bas(\n",
    "    NUM_POINTS,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    lr=LR,\n",
    "    prior=PRIOR,\n",
    "    prior_param1=MEAN,\n",
    "    prior_param2=TEMPERATURE,\n",
    ")\n",
    "ANI.save(f\"outputs/batch_shaping_{PRIOR.name.lower()}_p1={MEAN}_p2={TEMPERATURE}.gif\", fps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b31555-4ec3-4316-8e9e-aecb9cbcb966",
   "metadata": {},
   "source": [
    "![batch_shaping_relaxed_bernoulli_p1=0.2_p2=0.25](outputs/batch_shaping_relaxed_bernoulli_p1=0.2_p2=0.25.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c991a-83d4-4f84-8d22-48e8dd160925",
   "metadata": {},
   "source": [
    "#### Example 2:  (Symmetric) Beta distribution with parameters a = 0.9 and b = 1 - a = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc873682-f239-4291-a40f-41ff8fd4f559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter ffmpeg unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 0120 / 0120] loss = 1.86e-02\n",
      "CPU times: user 33.8 s, sys: 609 ms, total: 34.4 s\n",
      "Wall time: 33.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NUM_EPOCHS = 120\n",
    "LR = 0.1\n",
    "PRIOR = Prior.SYMBETA\n",
    "MEAN = 0.9\n",
    "\n",
    "ANI = train_bas(NUM_POINTS, num_epochs=NUM_EPOCHS, lr=LR, prior=PRIOR, prior_param1=MEAN)\n",
    "ANI.save(f\"outputs/batch_shaping_{PRIOR.name.lower()}_p1={MEAN}.gif\", fps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0cc65-57b3-46d4-a7f4-8852f8e5800d",
   "metadata": {},
   "source": [
    "![batch_shaping_symbeta_p1=0.9.gif](outputs/batch_shaping_symbeta_p1=0.9.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634bb168-47be-4ed4-a451-759484c4baa5",
   "metadata": {},
   "source": [
    "#### Example 3: Beta distribution with parameters a = 2 and b = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88e27aa4-fb5b-44ea-a808-4e7f1c810a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter ffmpeg unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 0100 / 0100] loss = 3.14e-06\n",
      "CPU times: user 28.4 s, sys: 577 ms, total: 29 s\n",
      "Wall time: 28.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NUM_EPOCHS = 100\n",
    "LR = 0.1\n",
    "PRIOR = Prior.BETA\n",
    "BETA_A = 2.0\n",
    "BETA_B = 6.0\n",
    "\n",
    "ANI = train_bas(\n",
    "    NUM_POINTS, num_epochs=NUM_EPOCHS, lr=LR, prior=PRIOR, prior_param1=BETA_A, prior_param2=BETA_B\n",
    ")\n",
    "ANI.save(f\"outputs/batch_shaping_{PRIOR.name.lower()}_p1={BETA_A}_p2={BETA_B}.gif\", fps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a8b587-90ae-4b4c-bac4-d520a885cafb",
   "metadata": {},
   "source": [
    "![batch_shaping_beta_p1=2_p2=6](outputs/batch_shaping_beta_p1=2_p2=6.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c718029-f90e-4ac3-a88d-7acbb5f83f37",
   "metadata": {},
   "source": [
    "## B. Comparison to the KL-divergence\n",
    "\n",
    "The KL divergence (or its symmetric extension, the Jensen-Shannon divergence) is a popular tool for matching density functions.\n",
    "\n",
    "$$\n",
    "KL(p, q) = \\sum_x p(x) \\log \\left(\\frac{p(x)}{q(x)}\\right)\n",
    "$$\n",
    "\n",
    "However, it requires estimating the empirical PDF of the data, which is non trivial for continuous data; Instead, the most common use cases of KL assumes a known parametric distribution on the data, typically the same distribution family as the target PDF ([see Pytorch's implementation of KL divergence for all supported distributions pairs](https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.kl)). In contrast, the Batch-Shaping loss directly estimates the empirical CDF of the data, which is easy to compute for both continuous or discrete data in practice.\n",
    "\n",
    "\n",
    "## C. Batch-Shaping for discrete data and finite support distribution\n",
    "\n",
    "Batch-Shaping can also be used to match discrete distributions. However, directly trying to match the correponsding CDF may sometimes lead to vanishing gradients: In fact, in parts of the space where the target probability density function is close or equal to zero (i.e., outside of the support), the gradient flowing to the data will have very low magnitudes, leading to slow training speeds. \n",
    "Therefore, it is important to tune training hyperparameters accordingly in some cases. We illustrate this potential issue in two examples below.\n",
    "\n",
    "\n",
    "#### Example 1: Edges of a Gaussian distribution\n",
    "We first take the example of a Gaussian distribution: Data points at the edges of the support where the density is almost zero do move towards where the mass of the distribution lies, but rather slowly. \n",
    "In the example, we improve training speed with a better choice of the initial clipping range to avoid regions with low density of the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf14da75-b3fc-4144-b090-cf037652df67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter ffmpeg unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 0200 / 0200] loss = 1.29e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter ffmpeg unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 0200 / 0200] loss = 1.79e-08\n",
      "CPU times: user 1min 52s, sys: 2.03 s, total: 1min 54s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NUM_EPOCHS = 200\n",
    "LR = 0.1\n",
    "PRIOR = Prior.NORMAL\n",
    "BETA_A = 0.6\n",
    "BETA_B = 0.1\n",
    "\n",
    "ANI = train_bas(\n",
    "    NUM_POINTS,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    lr=LR,\n",
    "    prior=PRIOR,\n",
    "    prior_param1=BETA_A,\n",
    "    prior_param2=BETA_B,\n",
    "    init_range=4,\n",
    ")\n",
    "ANI.save(\"outputs/test_outside_support.gif\", fps=10)\n",
    "\n",
    "ANI = train_bas(\n",
    "    NUM_POINTS,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    lr=LR,\n",
    "    prior=PRIOR,\n",
    "    prior_param1=BETA_A,\n",
    "    prior_param2=BETA_B,\n",
    "    init_range=1,\n",
    ")\n",
    "ANI.save(\"outputs/test_inside_support.gif\", fps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d479f2df-ea9e-455c-aef3-3794486ecdf9",
   "metadata": {},
   "source": [
    "**Points lying in low density regions**\n",
    "\n",
    "![test_outside_support](outputs/test_outside_support.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c42732-e396-4669-8d36-a08fec3817c9",
   "metadata": {},
   "source": [
    "**Better initialization to avoid low density regions**\n",
    "\n",
    "![test_inside_support](outputs/test_inside_support.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620face-afcc-448c-a977-a55d036c391c",
   "metadata": {},
   "source": [
    "#### Example 2: Relaxed Bernouli with low temperature\n",
    "\n",
    "Another example is the Relaxed Bernoulli distribution (aka Binary Concrete): When the temperature $t \\rightarrow 0$, the distribution becomes closer to the discrete Bernoulli distribution. In this setting, we do not observe any particular issue regarding training speed, even at low temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf9e43a8-8302-4519-9efd-77e893742731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter ffmpeg unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 0200 / 0200] loss = 2.70e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter ffmpeg unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 0200 / 0200] loss = 1.65e-01\n",
      "CPU times: user 1min 51s, sys: 1.98 s, total: 1min 53s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NUM_EPOCHS = 200\n",
    "LR = 0.1\n",
    "PRIOR = Prior.RELAXED_BERNOULLI\n",
    "MEAN = 0.8\n",
    "\n",
    "T1 = 0.4\n",
    "ANI = train_bas(\n",
    "    NUM_POINTS, num_epochs=NUM_EPOCHS, lr=LR, prior=PRIOR, prior_param1=MEAN, prior_param2=T1\n",
    ")\n",
    "ANI.save(\"outputs/test_high_temperature.gif\", fps=10)\n",
    "\n",
    "T2 = 0.01\n",
    "ANI = train_bas(\n",
    "    NUM_POINTS, num_epochs=NUM_EPOCHS, lr=LR, prior=PRIOR, prior_param1=MEAN, prior_param2=T2\n",
    ")\n",
    "ANI.save(\"outputs/test_low_temperature.gif\", fps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade32a99-d2c9-40a8-b3c0-64a815f2ce3a",
   "metadata": {},
   "source": [
    "### With higher temperature $t = 0.4$\n",
    "\n",
    "![test_hig_temperature](outputs/test_high_temperature.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5546d328-b55f-4cff-b33b-81d28966f879",
   "metadata": {},
   "source": [
    "### With lower temperature $t = 0.01$\n",
    "\n",
    "![test_low_temperature](outputs/test_low_temperature.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
